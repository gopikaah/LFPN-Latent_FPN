{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading sample image\n",
    "\n",
    "# import torchvision.io\n",
    "# import os\n",
    "# data_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\VOC2012'  # Path to the Pascal VOC dataset\n",
    "# image_id = '2007_000032' \n",
    "# image_file_path = os.path.join(data_dir, 'JPEGImages', f'{image_id}.jpg')\n",
    "# image = torchvision.io.read_image(image_file_path)\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # displaying 10 images sample code\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# # Replace these paths with your dataset paths\n",
    "# voc_images_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\JPEGImages'\n",
    "# voc_annotations_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\Annotations'\n",
    "\n",
    "# # Get the list of image filenames\n",
    "# image_filenames = os.listdir(voc_images_dir)[:10]\n",
    "\n",
    "# for image_filename in image_filenames:\n",
    "#     # Load image\n",
    "#     image_path = os.path.join(voc_images_dir, image_filename)\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     # Load corresponding annotation\n",
    "#     annotation_filename = image_filename.replace('.jpg', '.xml')\n",
    "#     annotation_path = os.path.join(voc_annotations_dir, annotation_filename)\n",
    "\n",
    "#     # Parse annotation XML\n",
    "#     tree = ET.parse(annotation_path)\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # Extract object labels if needed\n",
    "#     object_labels = []\n",
    "#     for obj in root.findall('object'):\n",
    "#         obj_label = obj.find('name').text\n",
    "#         object_labels.append(obj_label)\n",
    "\n",
    "#     print(f\"Image: {image_filename}, Object Labels: {object_labels}\")\n",
    "\n",
    "#     # # Display the image (you can remove this line if not needed)\n",
    "#     # cv2.imshow('Image', image)\n",
    "#     # cv2.waitKey(0)\n",
    "#     # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# # Replace these paths with your dataset paths\n",
    "# voc_images_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\JPEGImages'\n",
    "\n",
    "# # Get the list of image filenames\n",
    "# image_filenames = os.listdir(voc_images_dir)[:10]\n",
    "\n",
    "# for image_filename in image_filenames:\n",
    "#     # Load image\n",
    "#     image_path = os.path.join(voc_images_dir, image_filename)\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     new_width = 800\n",
    "#     new_height = 600\n",
    "#     resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # # Display the original and resized images (you can remove this if not needed)\n",
    "    # cv2.imshow('Original Image', image)\n",
    "    # cv2.imshow('Resized Image', resized_image)\n",
    "    \n",
    "    # # Wait for a key press and close the displayed windows\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pyramid\n",
    "import cv2\n",
    "def create_image_pyramid(image, num_levels):\n",
    "    pyramid = [image]  # Initialize the image pyramid with the original image\n",
    "    temp_image = image.copy()\n",
    "\n",
    "    for i in range(num_levels - 1):\n",
    "        # Downsample the image by half using simple resizing\n",
    "        temp_image = cv2.resize(temp_image, (temp_image.shape[1] // 2, temp_image.shape[0] // 2))\n",
    "        pyramid.append(temp_image)\n",
    "\n",
    "    return pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading image, created image pyramid\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# # Replace these paths with your dataset paths\n",
    "# voc_images_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\JPEGImages'\n",
    "\n",
    "# # Get the list of image filenames\n",
    "# image_filenames = os.listdir(voc_images_dir)[:10]\n",
    "# image_pyramids = []\n",
    "# for image_filename in image_filenames:\n",
    "#     # Load image\n",
    "#     image_path = os.path.join(voc_images_dir, image_filename)\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     new_width = 900\n",
    "#     new_height = 600\n",
    "#     resized_image = cv2.resize(image, (new_width, new_height))\n",
    "#     temp = create_image_pyramid(resized_image, 5)\n",
    "#     image_pyramids.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image_pyramids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def extract_features(image):\n",
    "    with torch.no_grad():\n",
    "        resnet.eval()  # Set the model to evaluation mode\n",
    "        features = resnet(image)\n",
    "    return features \n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "# Step 2: Remove the classification head (fully connected layer)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-2])  # Remove the last two layers (avgpool and fc)\n",
    "\n",
    "# Step 3: Replace global pooling with adaptive pooling\n",
    "resnet.add_module('adaptive_pool', nn.AdaptiveAvgPool2d(1))  # Preserve the spatial dimensions of the feature map\n",
    "\n",
    "# Step 4: Freeze the model's weights\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def process_image(image):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convert NumPy array to PIL Image\n",
    "        # transforms.Resize((224, 224)),  # ResNet input size is usually 224x224\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    image = torch.unsqueeze(image, 0)  # Add batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for Level 0: torch.Size([1, 2048, 600, 900])\n",
      "Features for Level 1: torch.Size([1, 2048, 300, 450])\n",
      "Features for Level 2: torch.Size([1, 2048, 150, 225])\n",
      "Features for Level 3: torch.Size([1, 2048, 75, 112])\n",
      "Features for Level 4: torch.Size([1, 2048, 37, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4423680000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m features \u001b[39m=\u001b[39m extract_features(image_torch)\n\u001b[0;32m     30\u001b[0m \u001b[39m# Resize the features to match the spatial dimensions of the image\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m features_resized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(features, size\u001b[39m=\u001b[39;49m(image\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], image\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     33\u001b[0m features_list\u001b[39m.\u001b[39mappend(features_resized)\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeatures for Level \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfeatures_resized\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3959\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3957\u001b[0m     \u001b[39mif\u001b[39;00m antialias:\n\u001b[0;32m   3958\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_upsample_bilinear2d_aa(\u001b[39minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m-> 3959\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bilinear2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[0;32m   3960\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3961\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4423680000 bytes."
     ]
    }
   ],
   "source": [
    "# Loading image, created image pyramid, Extracting feature using resnet\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Replace these paths with your dataset paths\n",
    "voc_images_dir = 'E:\\DUK\\Summer Internship\\Dataset\\pascal VOC\\JPEGImages'\n",
    "\n",
    "# Get the list of image filenames\n",
    "image_filenames = os.listdir(voc_images_dir)[:10]\n",
    "image_pyramids = []\n",
    "for image_filename in image_filenames:\n",
    "    # Load image\n",
    "    image_path = os.path.join(voc_images_dir, image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    new_width = 900\n",
    "    new_height = 600\n",
    "    resized_image = cv2.resize(image, (new_width, new_height))\n",
    "    temp = create_image_pyramid(resized_image, 5)\n",
    "    image_pyramids.append(temp)\n",
    "\n",
    "full_features = []\n",
    "for image_pyramid in image_pyramids:\n",
    "    features_list = []\n",
    "    for i, image in enumerate(image_pyramid):\n",
    "        image_torch = process_image(image)  # Convert the image to PyTorch format\n",
    "        features = extract_features(image_torch)\n",
    "        \n",
    "        # Resize the features to match the spatial dimensions of the image\n",
    "        features_resized = torch.nn.functional.interpolate(features, size=(image.shape[0], image.shape[1]), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        features_list.append(features_resized)\n",
    "        print(f\"Features for Level {i}: {features_resized.shape}\")\n",
    "    full_features.append(features_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
